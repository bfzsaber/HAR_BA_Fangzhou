# 12.07.23
- original finn-hls code: [HLS 214-309] in function 'void ConvolutionInputGenerator_1D<1u, 20u, 8u, 18u, 18u, 1u, 10u, ap_resource_dflt>(hls::stream<ap_uint<(10u) * (8u)>, 0>&, hls::stream<ap_uint<(10u) * (8u)>, 0>&, unsigned int, ap_resource_dflt const&)': Detected unsupported array/vector as field with size 0 in struct object 'buffer' ([slidingwindow.h](https://github.com/Xilinx/finn-hlslib/blob/master/slidingwindow.h):1880:6)
- problem code: constexpr unsigned  BUFFER_SIZE = (ConvKernelDim_x - 1) * SIMD_COUNT; finn conv1d and hls may expect input in shape (1,n) and kernel in shape (1, n)
- solution: delete -1, since other ConvolutionInputGenerator_1D methods do not have it.----> need to verify!
## Brevitas Export:
(  cross_channel_interaction_type = "attn",
                      cross_channel_aggregation_type = "FC",
                      temporal_info_interaction_type = "conv",
                      temporal_info_aggregation_type = "tnaive")
- replace tanh with ReLU as "Module <class 'brevitas.nn.quant_activation.QuantTanh'> not supported for export." even though QuantTanh class is included in [quant_activation](https://github.com/Xilinx/brevitas/blob/master/src/brevitas/nn/quant_activation.py)
- replace GELU with ReLU
- in  temporal_conv_1d: delete (padding="same", padding_mode="replicate") as in [quant_conv](https://github.com/Xilinx/brevitas/blob/master/src/brevitas/nn/quant_conv.py) assert self.padding_mode == 'zeros'
        assert not (padding_type == 'same' and padding != 0)
#
next week: (based on the limitation of FINN, designing plans and deadlines to solve the problem) plan a: Model Partition - which node? How?

- TinyHAR differs from single chain models from FINN examples and FINN's Target networks (what I studied before)
- similar project a [Architectural exploration and efficient FPGA implementation of convolutional neural networks](https://webthesis.biblio.polito.it/17899/)
- similar project b [Integration of a convolutional neural network for speech-to-text recognition in an FPGA compiler flow](https://repository.tudelft.nl/islandora/object/uuid%3Ab6c889b1-e06f-447c-af69-55708555bf90)





# 09.06.2023

[lower_convs_to_matmul](https://github.com/fastmachinelearning/qonnx/blob/main/src/qonnx/transformation/lower_convs_to_matmul.py) uses reshape, but according to [tfc_end2end_example.ipynb](https://github.com/Xilinx/finn/blob/main/notebooks/end2end_example/bnn-pynq/tfc_end2end_example.ipynb) "you can see that there is a mixture of FINN HLS layers (MatrixVectorActivation and Thresholding_Batch) with one regular ONNX layers (**Reshape)**."

_______

## reshape

**FINN Tutorials**

[0. Analysis Passes](https://github.com/Xilinx/finn/blob/main/notebooks/advanced/0_custom_analysis_pass.ipynb)

example # nodes of the same operation types

[1.Transformation Passes](https://github.com/Xilinx/finn/blob/main/notebooks/advanced/1_custom_transformation_pass.ipynb)

example: ConvertSubToAdd. Requirement of parallel transformation

[2.Custom Operation](https://github.com/Xilinx/finn/blob/main/notebooks/advanced/2_custom_op.ipynb)

example: Python and C++ code for exponent calculation

**these custom operations are generic** and not really tied to e.g. Vitis HLS or few-bit quantization.

It's possible to provide arbitrary Python/C/C++/... execution and code generation paths for custom nodes.

____

[lower_convs_to_matmul.py](https://github.com/fastmachinelearning/qonnx/blob/main/src/qonnx/transformation/lower_convs_to_matmul.py)

(1.Step. Transformation)

[[finn-hlslib](https://github.com/Xilinx/finn-hlslib)/**convlayer.h**](https://github.com/Xilinx/finn-hlslib/blob/master/convlayer.h)

  
  

# 02.06.2023

  

  

- fixed simd folding factors, SIMD must divide IFMChannels for Conv Sliding windows (ConvolutionInputGenerator1D for our case, not 2D in example)

  

  

-

  

  

reason why only works for permutation 0 3 1 2: considered to work for ONNX OPs assume NCHW format and HLS NHWC

  

  

- [AbsorbTransposeIntoMultiThreshold](https://finn.readthedocs.io/en/latest/source_code/finn.transformation.streamline.html#finn.transformation.streamline.absorb.AbsorbTransposeIntoMultiThreshold)

  

  

- [AbsorbTransposeIntoResize](https://finn.readthedocs.io/en/latest/source_code/finn.transformation.streamline.html#finn.transformation.streamline.absorb.AbsorbTransposeIntoResize)

  

  

***

  

  

Ideas from **similar project:** [Integration of a convolutional neural network for speech-to-text recognition in an FPGA compiler flow](https://repository.tudelft.nl/islandora/object/uuid%3Ab6c889b1-e06f-447c-af69-55708555bf90)

  

  

based on [QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions](https://paperswithcode.com/paper/quartznet-deep-automatic-speech-recognition)

  

  

[**CNN code**](https://github.com/isadrtdinov/quartznet/blob/main/asr/models/quartznet.py)

  

  

  

- no permutation reshape in cnn model

  

  

- due to conv blocks to sparse MatMul, may exeeds the max. capacity, and also lead to large run time when executing the model. ---------> partition the model into multiple smaller onnx sub-models ( *not seen in finn example notebooks* )

  

  

- if implement manual partition, works for varification( simulating c++ kernels and performing RTL simulation) but not sure with real FPGA, and the workflow with each part, how to pack after etc,

  

  

  

# 19.05.2023

  

  

  

### problem: zedboard getting started

  

  

  

[Getting Started Guide](https://www.avnet.com/wps/wcm/connect/onesite/7ae0f288-1cc5-4283-9e85-300c5401b680/GS-AES-Z7EV-7Z020-G-V7-1.pdf?MOD=AJPERES&CACHEID=ROOTWORKSPACE.Z18_NA5A1I41L0ICD0ABNDMDDG0000-7ae0f288-1cc5-4283-9e85-300c5401b680-nxyWIEs

  

  

  

)

  

  

  

[linux - zedboard issues](https://github.com/ucb-bar/fpga-zynq/issues/77)

  

  

  

arm cores?

  

  

  

  

# 05.05.2023

  

  

  

  

- Hardware build ERROR: [HLS 200-1715] Encountered problem during source synthesis

  

  

  

  

[see xilinx](https://support.xilinx.com/s/question/0D52E00006jqpkeSAA/vitis-hls-20211-error-hls-2001715-encountered-problem-during-source-synthesis?language=en_US)

  

  

  

  

# 28.04.2023

  

  

  

  

  

  

- fixed topK problem, see [brevitas tutorial](https://xilinx.github.io/brevitas/tutorials/tvmcon2021.html#) about Quantizer, QuantTensor etc

  

  

  

  

  

  

- try to use AbsorbTransposeIntoMultiThreshold: only works for **perms == [0, 3, 1, 2]**, added QuantIdentity, but got

  

  

  

  

  

  

**AssertionError: Signed output requres actval < 0**

  

  

  

  

  

  

[unsolved issue](https://github.com/Xilinx/finn/issues/337)

  

  

  

  

  

  

[labeld bug](https://github.com/Xilinx/finn/issues/682)

  

  

  

  

  

  

  

### transformation for **transpose**

  

  

  

  

  

  

1. AbsorbConsecutiveTransposes: only for opposite transposes

  

  

  

  

  

  

2. AbsorbTransposeIntoFlatten

  

  

  

  

  

  

3. AbsorbTransposeIntoMultiThreshold (**failed**)

  

  

  

  

  

  

4. AbsorbTransposeIntoResize: only works for **perms == [0, 3, 1, 2]**

  

  

  

  

  

  

#

  

  

  

  

  

  

therefor **still unsolved** AssertionError for original mcnn: cycle-free graph violated: partition depends on itself

  

  

  

  

  

  

[see discussions 1](https://github.com/Xilinx/finn/discussions/431)

  

  

  

  

  

  

[see discussions 2](https://github.com/Xilinx/finn/discussions/495)

  

  

  

  

  

  

## after changing fc layers

  

  

  

  

  

  

[example cnn model from FINN](https://github.com/Xilinx/brevitas/blob/master/src/brevitas_examples/bnn_pynq/models/CNV.py) has no reshape and permutaion --> changed mcnn logic (**to do** shape from config)

  

  

  

  

  

  

  

# 21.04.2023

  

  

  

  

  

  

  

  

[example cnn model from FINN](https://github.com/Xilinx/brevitas/blob/master/src/brevitas_examples/bnn_pynq/models/CNV.py)

  

  

  

  

  

  

  

  

### known info to costumize FINN transformation

  

  

  

  

  

  

  

  

- ALL of the supported HLS Kernels ih the back-end of FINN assume integer format inputs **(use is_integer())**

  

  

  

  

  

  

  

  

- [convert_to_hls_layers.py](https://github.com/Xilinx/finn/blob/main/src/finn/transformation/fpgadataflow/convert_to_hls_layers.py) InferLabelSelectLayer(Transformation): Convert any TopK into a LabelSelect HLS layer. # skip conversion for layers with float input # skip conversion for if value output is connected (not supported)

  

  

  

  

  

  

  

  

#

  

  

  

  

  

  

  

  

**to do**

  

  

  

  

  

  

  

  

- output of mcnn should pass is_integer (use QuantIdendity, evaluation should rewrite, cause output is QuantTensor, need more information)

  

  

  

  

  

  

  

[brevitas tutorial](https://xilinx.github.io/brevitas/tutorials/quant_tensor_quant_conv2d_overview.html)

  

  

  

  

  

  

  

  

- fixed multithreshold and transformation oder (see onnx files)

  

  

  

  

  

  

  

- transpose itself cannot be converted into HLS layer see

  

  

  

  

  

  

  

  

# 14.04.2023

  

  

  

  

  

  

  

  

  

solved: conv transformation

  

  

  

  

  

  

  

  

  

***

  

  

  

  

  

  

  

  

  

## Problem in Partitioning, Conversion to HLS Layers and Folding

  

  

  

  

  

  

  

  

  

AssertionError: cycle-free graph violated: partition depends on itself<br>

  

  

  

  

  

  

  

  

  

[see](https://github.com/Xilinx/finn/discussions/431)<br>

  

  

  

  

  

  

  

  

  

[see](https://github.com/Xilinx/finn/discussions/495)<br>

  

  

  

  

  

  

  

  

  

(all Maxpool problem, not useful in our case)

  

  

  

  

  

  

  

  

  

**suspected Problem:**

  

  

  

  

  

  

  

  

  

FINN uses almost exclusively NHWC operators, while most onnx ops prefer NCHW.[see issue](https://github.com/Xilinx/finn/issues/136)<br>

  

  

  

  

  

  

  

  

  

**changed** [infer_data_layouts.py](https://github.com/fastmachinelearning/qonnx/blob/main/src/qonnx/transformation/infer_data_layouts.py): "Try to infer data layout annotations info for all input/intermediate/output tensors based on inputs and node type."

  

  

  

  

  

  

  

  

  

changed default NCWH to NHWC

  

  

  

  

  

  

  

  

  

based on notation # first, make sure that the global input has an annotation this is really hard to do in general, so we do some bad guesswork

  

  

  

  

  

  

  

  

  

- `finn.util.onnx.``nchw_to_nhwc`(_t_, _model_, _idx_, _reverse=False_)

  

  

  

  

  

  

  

  

  

[see](https://finn-base.readthedocs.io/en/latest/api/finn.util.onnx.html#finn.util.onnx.nchw_to_nhwc "Permalink to this definition")

  

  

  

  

  

  

  

  

  

Converts between NCHW <-> NHWC layouts for tensor t by inserting a transpose. If reverse=False, t is assumed NCHW and we insert transpose to convert NCHW -> NHWC If reverse=True, t is assumed NHWC and we insert transpose to convert NHWC -> NCHW.

  

  

  

  

  

  

  

  

  

***

  

  

  

  

  

  

  

  

  

### to do

  

  

  

  

  

  

  

  

  

- Not all ONNX nodes have their data layout set

  

  

  

  

  

  

  

  

  

- [finn index doc](https://finn.readthedocs.io/en/latest/genindex.html), which node can transform to hls layer? (example: flatten?)

  

  

  

  

  

  

  

  

  

- read **similar project:** [Integration of a convolutional neural network for speech-to-text recognition in an FPGA compiler flow](https://repository.tudelft.nl/islandora/object/uuid%3Ab6c889b1-e06f-447c-af69-55708555bf90)

  

  

  

  

  

  

  

  

  

  

# 07.04.2023

  

  

  

  

  

  

  

  

  

  

## unknown Problem in Lowering and Streamlining (14.04 AbsorbScalarMulAddIntoTopK works only for scalar Mul and Add)

  

  

  

  

  

  

  

  

  

  

[see](https://github.com/Xilinx/finn/discussions/431): The final nodes (Mul, Add, TopK) are not converted to HLS node<br>

  

  

  

  

  

  

  

  

  

  

[tfc_end2end_example.ipynb](https://github.com/Xilinx/finn/blob/main/notebooks/end2end_example/bnn-pynq/tfc_end2end_example.ipynb): "...a mixture of FINN HLS layers (MatrixVectorActivation and Thresholding_Batch) with one regular ONNX layers (Reshape). **To create a bitstream, FINN needs a model with only HLS layers.** In order to achieve this, we will use the `CreateDataflowPartition` transformation to create a "dataflow partition" in this graph, separating out the HLS layers into another model, and replacing them with a placeholder layer called StreamingDataflowPartition."

  

  

  

  

  

  

  

  

  

  

  

## Problem Known in Conversion to HLS Layers

  

  

  

  

  

  

  

  

  

  

**14.04 solved by edit finn: 1D images and/or kernels logic [see](https://github.com/Xilinx/finn/blob/b3bdff118ae076cb776af6e51ddc28eeaa0d6390/src/finn/transformation/fpgadataflow/convert_to_hls_layers.py#L83)**

  

  

  

  

  

  

  

  

  

  

create equivalent ConvolutionInputGenerator node

  

  

  

  

  

  

  

  

  

  

input MCNN:(1,1,169,18)

  

  

  

  

  

  

  

  

  

  

ifm_dim_h = i2c_in_shape[1]

  

  

  

  

  

  

  

  

  

  

ifm_dim_w = i2c_in_shape[2]

  

  

  

  

  

  

  

  

  

  

  

is_square_image = ConvInpGen_idim_h == ConvInpGen_idim_w

  

  

  

  

  

  

  

  

  

  

if(is_square_image

  

  

  

  

  

  

  

  

  

  

and is_square_kernel)

  

  

  

  

  

  

  

  

  

  

assert is_equal_strid e= stride_h == stride_w

  

  

  

  

  

  

  

  

  

  

conv2d->ConvolutionInputGenerator

  

  

  

  

  

  

  

  

  

  

ConvolutionInputGenerator1D:

  

  

  

  

  

  

  

  

  

  

**is_1d_convolution = (k_h == 1 and k_w > 1 and ifm_dim_h == 1) or (k_h > 1 and k_w == 1 and ifm_dim_w == 1)**<br>

  

  

  

  

  

  

  

  

  

  

[see](https://github.com/Xilinx/finn/blob/b3bdff118ae076cb776af6e51ddc28eeaa0d6390/src/finn/transformation/fpgadataflow/convert_to_hls_layers.py#L83)

  

  

  

  

  

  

  

  

  

  

  

## About Folding Factors

  

  

  

  

  

  

  

  

  

  

**todo:choose paeameters that maximise throughput while staying in the resource bound of target device**

  

  

  

  

  

  

  

  

  

  

  

[set_folding.py](https://github.com/Xilinx/finn/blob/main/src/finn/transformation/fpgadataflow/set_folding.py)<br>

  

  

  

  

  

  

  

  

  

  

  

[set_fifo_depths.py](https://github.com/Xilinx/finn/blob/main/src/finn/transformation/fpgadataflow/set_fifo_depths.py)<br>

  

  

  

  

  

  

  

  

  

  

  

  

- For each HLSCustomOp node type, the attribute may vary but is typically one of {PE, SIMD}

  

  

  

  

  

  

  

  

  

  

  

  

- Notable exceptions and special behavior:

  

  

  

  

  

  

  

  

  

  

  

1. When folding dense convolution/FC compute engines ("MVAU"/MatrixVectorActivation), which have two attributes (PE and SIMD):

  

  

  

  

  

  

  

  

  

  

  

-first increases SIMD while weight stream width per PE is <= mvau_wwidth_max (configurable in the SetFolding initializer, defaults to 36)

  

  

  

  

  

  

  

  

  

  

  

-then increases PE until the target is met or max PE reached

  

  

  

  

  

  

  

  

  

  

  

2. When folding depthwise convolutions ("VVAU"/VectorVectorActivation) or spatial reduction ops (Pool_Batch):

  

  

  

  

  

  

  

  

  

  

  

-the producer of the node is expected to be a ConvolutionInputGenerator with depthwise=1, whose SIMD value will be set equal to the PE value of its consumer node

  

  

  

  

  

  

  

  

  

  

  

[tfc_end2end_example.ipynb](https://github.com/Xilinx/finn/blob/main/notebooks/end2end_example/bnn-pynq/tfc_end2end_example.ipynb)<br>

  

  

  

  

  

  

  

  

  

  

  

| name|MH|MW|PE|SIMD|

  

  

  

  

  

  

  

  

  

  

| :---: | :---: | :---: | :---: | :---: |

  

  

  

  

  

  

  

  

  

  

| MatMul_0 |64|784|16|49|

  

  

  

  

  

  

  

  

  

  

| MatMul_1 |64|64|8|8|

  

  

  

  

  

  

  

  

  

  

| MatMul_2 |64|64|8|8|

  

  

  

  

  

  

  

  

  

  

| MatMul_3 |10|64|10|8|

  

  

  

  

  

  

  

  

  

  

<br>

  

  

  

  

  

  

  

  

  

  

  

[cnv_end2end_example.ipynb](https://github.com/Xilinx/finn/blob/main/notebooks/end2end_example/bnn-pynq/cnv_end2end_example.ipynb)<br>

  

  

  

  

  

  

  

  

  

  

  

| name|MH|MW|PE|SIMD|in_fifo_depth|

  

  

  

  

  

  

  

  

  

  

| :---: | :---: | :---: | :---: | :---: | :---: |

  

  

  

  

  

  

  

  

  

  

| MatrixVectorActivation_0 |64|27|16|3|128

  

  

  

  

  

  

  

  

  

  

| MatrixVectorActivation_1 |64|576|32|32|128

  

  

  

  

  

  

  

  

  

  

| MatrixVectorActivation_2 |128|576|16|32|128

  

  

  

  

  

  

  

  

  

  

| MatrixVectorActivation_3 |128|1152|16|32|128

  

  

  

  

  

  

  

  

  

  

| MatrixVectorActivation_4 |256|1152|4|32|81

  

  

  

  

  

  

  

  

  

  

| MatrixVectorActivation_5 |256|2304|1|32|2
