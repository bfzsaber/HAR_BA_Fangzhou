{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import onnxruntime (again) to avoid version error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx \n",
    "import torch \n",
    "import onnxruntime\n",
    "import os\n",
    "    \n",
    "build_dir = os.environ[\"FINN_BUILD_DIR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version check for debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n",
      "1.11.1\n"
     ]
    }
   ],
   "source": [
    "print(onnx.__version__)\n",
    "print(onnxruntime.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import model into FINN with ModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bian/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:93: UserWarning: Some old-style domain attributes were automatically converted to new-style,\n",
      "                i.e. domain=finn to domain=qonnx.custom_op.<general|fpgadataflow|...>\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.util.cleanup import cleanup_model\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "ready_model_filename = \"pamp2_attn_conv_4_mod.onnx\"\n",
    "model_for_sim = ModelWrapper(ready_model_filename)\n",
    "\n",
    "model_for_sim = cleanup_model(model_for_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_equal_nodes(model):\n",
    "    count_dict = {}\n",
    "    for node in model.graph.node:\n",
    "        if node.op_type in count_dict:\n",
    "            count_dict[node.op_type] +=1\n",
    "        else:\n",
    "            count_dict[node.op_type] = 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MultiThreshold': 15, 'Add': 45, 'Mul': 171, 'Conv': 6, 'BatchNormalization': 4, 'Transpose': 148, 'Gather': 37, 'MatMul': 184, 'Softmax': 37, 'Unsqueeze': 37, 'Concat': 1, 'Reshape': 1, 'Squeeze': 1, 'ReduceSum': 1}\n"
     ]
    }
   ],
   "source": [
    "print(model_for_sim.analysis(count_equal_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.visualization import showInNetron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'pamp2_attn_conv_4_mod.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f30e09fb520>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"pamp2_attn_conv_4_mod.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Network preparation: Tidy-up transformations <a id=\"network_preparations\"></a>\n",
    "\n",
    "all the intermediate tensors need to have statically defined shapes.\n",
    "\n",
    "These transformations are:\n",
    "\n",
    "- GiveUniqueNodeNames\n",
    "- GiveReadableTensorNames\n",
    "- InferShapes\n",
    "- InferDataTypes\n",
    "- FoldConstants\n",
    "- RemoveStaticGraphInputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "model_for_sim = model_for_sim.transform(InferShapes())\n",
    "model_for_sim = model_for_sim.transform(FoldConstants())\n",
    "model_for_sim = model_for_sim.transform(GiveUniqueNodeNames())\n",
    "model_for_sim = model_for_sim.transform(GiveReadableTensorNames())\n",
    "model_for_sim = model_for_sim.transform(InferDataTypes())\n",
    "model_for_sim = model_for_sim.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "tidy_model_filename = \"pamp2_tidy.onnx\"\n",
    "model_for_sim.save(tidy_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'pamp2_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f30e09fb460>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(tidy_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Pre- and Postprocessing <a id='prepost'></a>\n",
    "\n",
    "Preprocessing and postprocessing steps can be added directly in the ONNX graph.  <span style=\"color:red\">\n",
    "In FINN, we can bake some of these pre/postprocessing operatings into the graph, and in some cases these can be highly beneficial for performance by allowing our accelerator to directly consume raw data instead of going through CPU preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model = ModelWrapper(tidy_model_filename)\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "chkpt_name = \"pamp2_pre_post.onnx\"\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(chkpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'pamp2_pre_post.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f30e09fb400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(chkpt_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manually partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add_1 9\n",
      "Transpose_0 35\n",
      "MatMul_3 76\n",
      "Transpose_145 649\n",
      "Reshape_0 650\n",
      "Transpose_147 667\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "manually partiton\n",
    "\"\"\"\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "for n in model.graph.node:\n",
    "    if n.name == 'Add_1':\n",
    "        add_1 = n\n",
    "        print(\"Add_1 \" + str(model_for_sim.get_node_index(n)))\n",
    "    elif n.name == 'MatMul_3':\n",
    "        matMul_3 = n\n",
    "        print(\"MatMul_3 \" + str(model_for_sim.get_node_index(n)))\n",
    "    elif n.name == 'Reshape_0':\n",
    "        reshape_0 = n\n",
    "        print(\"Reshape_0 \" + str(model_for_sim.get_node_index(n)))\n",
    "    elif n.name == 'Transpose_0':\n",
    "        transpose_0 = n\n",
    "        print(\"Transpose_0 \" + str(model_for_sim.get_node_index(n)))\n",
    "    elif n.name == 'Transpose_145':\n",
    "        transpose_145 = n\n",
    "        print(\"Transpose_145 \" + str(model_for_sim.get_node_index(n)))\n",
    "    elif n.name == 'Transpose_147':\n",
    "        transpose_147 = n\n",
    "        print(\"Transpose_147 \" + str(model_for_sim.get_node_index(n)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save as file /home/bian/finn/notebooks/TinyHAR/e.PAMP2/partition_0.onnx\n",
      "save as file /home/bian/finn/notebooks/TinyHAR/e.PAMP2/partition_1.onnx\n"
     ]
    }
   ],
   "source": [
    "#partition_dir\n",
    "import qonnx.transformation.create_generic_partitions as partition\n",
    "import finn.transformation.fpgadataflow.cleanup as cleanup\n",
    "\n",
    "# Set partition_dir to the current directory\n",
    "partition_dir = \"/home/bian/finn/notebooks/TinyHAR/e.PAMP2\"\n",
    "\n",
    "model = ModelWrapper(chkpt_name)\n",
    "model = model.transform(cleanup.CleanUp())\n",
    "model = model.transform(InferShapes())\n",
    "\n",
    "model = model.transform(partition.PartitionFromDict({0 : range(0, 35), 1 : range(649, 667)}, partition_dir = partition_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"pamp2_partition.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Lowering and Streamlining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bian/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:120: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "import finn.transformation.streamline.reorder as reorder\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "\n",
    "model = ModelWrapper(\"partition_0.onnx\")\n",
    "model = model.transform(Streamline())\n",
    "#conv1d, shape kernel not 2d\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "#TinyHAR has no Maxpool\n",
    "#model = model.transform(MakeMaxPoolNHWC())\n",
    "\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "\n",
    "#model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(Streamline())\n",
    "#absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "#infer_data_layouts.py:119: UserWarning: Assuming 4D input is NCHW\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "model.save(\"pamp2_part0_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'pamp2_part0_streamlined.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f30e099b340>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"pamp2_part0_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Partitioning, Conversion to HLS Layers and Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.streamline import Streamline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node MultiThreshold_0 input not integer\n",
      "node MultiThreshold_0 input not integer\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"decoupled\"\n",
    "model = ModelWrapper(\"pamp2_part0_streamlined.onnx\")\n",
    "\n",
    "model = model.transform(to_hls.InferBinaryMatrixVectorActivation(mem_mode))\n",
    "model = model.transform(to_hls.InferQuantizedMatrixVectorActivation(mem_mode))\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hls.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hls.InferThresholdingLayer())\n",
    "model = model.transform(to_hls.InferConvInpGen())\n",
    "#model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "# get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "model = model.transform(RemoveCNVtoFCFlatten())\n",
    "# get rid of Tranpose -> Tranpose identity seq\n",
    "model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "\n",
    "# infer tensor data layouts\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bian/finn/src/finn/transformation/fpgadataflow/convert_to_hls_layers.py:1332: UserWarning: Broadcasting Mul(Mul_0)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = model.transform(to_hls.InferChannelwiseLinearLayer())\n",
    "model = model.transform(to_hls.InferLabelSelectLayer())\n",
    "model = model.transform(GiveUniqueNodeNames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save as file /tmp/finn_dev_bian/dataflow_partition_9vtmbrbq/partition_0.onnx\n"
     ]
    }
   ],
   "source": [
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(\"pamp2_part0_parent.onnx\")\n",
    "\n",
    "\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(\"pamp2_part0_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'pamp2_part0_parent.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f30e09fbe80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"pamp2_part0_parent.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'pamp2_part0_dataflow_model.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f30df179490>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"pamp2_part0_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(\"pamp2_part0_dataflow_model.onnx\")\n",
    "fc_layers = model.get_nodes_by_op_type(\"MatrixVectorActivation\")\n",
    "# each tuple is (PE, SIMD, in_fifo_depth,ramstyle) for a layer\n",
    "folding = [\n",
    "    (1, 1, 64, \"auto\"),\n",
    "    (5, 10, 64, \"auto\"),\n",
    "    (5, 10, 64, \"auto\"),\n",
    "    (5, 10, 64, \"auto\")\n",
    "]\n",
    "for fcl, (pe, simd, ififodepth,ramstyle) in zip(fc_layers, folding):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepth\", ififodepth)\n",
    "    fcl_inst.set_nodeattr(\"ram_style\", ramstyle)\n",
    "\n",
    "# use same SIMD values for the sliding window operators\n",
    "swg_layers = model.get_nodes_by_op_type(\"ConvolutionInputGenerator1D\")\n",
    "for i in range(len(swg_layers)):\n",
    "    swg_inst = getCustomOp(swg_layers[i])\n",
    "    simd = folding[i][1]\n",
    "    swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    swg_inst.set_nodeattr(\"ram_style\", ramstyle)\n",
    "\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model.save(\"pamp2_part0_dataflow_folded.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'pamp2_part0_dataflow_folded.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f30e09ffac0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"pamp2_part0_dataflow_folded.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Hardware Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Ultra96', 'Pynq-Z1', 'Pynq-Z2', 'ZCU102', 'ZCU104', 'ZCU111', 'RFSoC2x2'])\n"
     ]
    }
   ],
   "source": [
    "# print the names of the supported PYNQ boards\n",
    "from finn.util.basic import pynq_part_map\n",
    "print(pynq_part_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save as file /tmp/finn_dev_bian/dataflow_partition_rdcg6ymv/partition_0.onnx\n",
      "save as file /tmp/finn_dev_bian/dataflow_partition_rdcg6ymv/partition_1.onnx\n",
      "save as file /tmp/finn_dev_bian/dataflow_partition_rdcg6ymv/partition_2.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bian/finn/src/finn/transformation/fpgadataflow/floorplan.py:108: UserWarning: 20 nodes have no entry in the provided floorplan, SLR was set to -1\n",
      "  warnings.warn(\n",
      "/home/bian/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:189: UserWarning: Overriding input FIFO depth to 32\n",
      "  warnings.warn(\"Overriding input FIFO depth to 32\")\n",
      "/home/bian/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:236: UserWarning: Overriding output FIFO depth to 32\n",
      "  warnings.warn(\"Overriding output FIFO depth to 32\")\n"
     ]
    }
   ],
   "source": [
    "test_pynq_board = \"ZCU104\"\n",
    "target_clk_ns = 10\n",
    "\n",
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "model = ModelWrapper(\"pamp2_part0_dataflow_folded.onnx\")\n",
    "model = model.transform(ZynqBuild(platform = test_pynq_board, period_ns = target_clk_ns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the `ZynqBuild` we run one additional transformation to generate a PYNQ driver for the accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "model = model.transform(MakePYNQDriver(\"zynq-iodma\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"pamp2_part0_synth.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataflow_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "import finn.analysis.fpgadataflow.dataflow_performance as performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous run results deleted!\n"
     ]
    }
   ],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_file = \"pamp2_part0_dataflow_folded.onnx\"\n",
    "\n",
    "estimates_output_dir = \"output_estimates_only\"\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(estimates_output_dir):\n",
    "    shutil.rmtree(estimates_output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "\n",
    "cfg_estimates = build.DataflowBuildConfig(\n",
    "    output_dir          = estimates_output_dir,\n",
    "    mvau_wwidth_max     = 80,\n",
    "    target_fps          = 1000000,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    fpga_part           = \"xc7z020clg400-1\",\n",
    "    steps               = build_cfg.estimate_only_dataflow_steps,\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataflow accelerator from pamp2_part0_dataflow_folded.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_bian\n",
      "Final outputs will be generated in output_estimates_only\n",
      "Build log is at output_estimates_only/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/8]\n",
      "Running step: step_tidy_up [2/8]\n",
      "Running step: step_streamline [3/8]\n",
      "Running step: step_convert_to_hls [4/8]\n",
      "Running step: step_create_dataflow_partition [5/8]\n",
      "Running step: step_target_fps_parallelization [6/8]\n",
      "Running step: step_apply_folding_config [7/8]\n",
      "Running step: step_generate_estimate_reports [8/8]\n",
      "Completed successfully\n",
      "CPU times: user 862 ms, sys: 32.5 ms, total: 894 ms\n",
      "Wall time: 822 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "build.build_dataflow_cfg(model_file, cfg_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_folding_config.json  intermediate_models  time_per_step.json\r\n",
      "build_dataflow.log\t  report\r\n"
     ]
    }
   ],
   "source": [
    "! ls {estimates_output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate_layer_config_alternatives.json  estimate_network_performance.json\r\n",
      "estimate_layer_cycles.json\t\t op_and_param_counts.json\r\n",
      "estimate_layer_resources.json\r\n"
     ]
    }
   ],
   "source": [
    "! ls {estimates_output_dir}/report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"critical_path_cycles\": 52510,\r\n",
      "  \"max_cycles\": 7200,\r\n",
      "  \"max_cycles_node_name\": \"MatrixVectorActivation_1\",\r\n",
      "  \"estimated_throughput_fps\": 13888.888888888889,\r\n",
      "  \"estimated_latency_ns\": 525100.0\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat {estimates_output_dir}/report/estimate_network_performance.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Lowering and StreamliningÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "import finn.transformation.streamline.reorder as reorder\n",
    "from qonnx.transformation.change_3d_tensors_to_4d import Change3DTo4DTensors\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "\n",
    "model = ModelWrapper(\"partition_1.onnx\")\n",
    "\n",
    "#conv 1d\n",
    "model = model.transform(Change3DTo4DTensors())\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "\n",
    "#conv1d, shape kernel not 2d\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "#TinyHAR has no Maxpool\n",
    "#model = model.transform(MakeMaxPoolNHWC())\n",
    "\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "\n",
    "#conv1d, shape kernel not 2d\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "#TinyHAR has no Maxpool\n",
    "#model = model.transform(MakeMaxPoolNHWC())\n",
    "\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model = model.transform(absorb.AbsorbAddIntoMultiThreshold())\n",
    "model = model.transform(absorb.AbsorbMulIntoMultiThreshold())\n",
    "\n",
    "#model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(Streamline())\n",
    "#absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "#infer_data_layouts.py:119: UserWarning: Assuming 4D input is NCHW\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "model.save(\"pamp2_part1_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Partitioning, Conversion to HLS Layers and Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"decoupled\"\n",
    "model = ModelWrapper(\"part1_streamlined.onnx\")\n",
    "\n",
    "model = model.transform(to_hls.InferBinaryMatrixVectorActivation(mem_mode))\n",
    "model = model.transform(to_hls.InferQuantizedMatrixVectorActivation(mem_mode))\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hls.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hls.InferThresholdingLayer())\n",
    "model = model.transform(to_hls.InferConvInpGen())\n",
    "#model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "# get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "model = model.transform(RemoveCNVtoFCFlatten())\n",
    "# get rid of Tranpose -> Tranpose identity seq\n",
    "model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "\n",
    "# infer tensor data layouts\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model = model.transform(to_hls.InferChannelwiseLinearLayer())\n",
    "model = model.transform(to_hls.InferLabelSelectLayer())\n",
    "model = model.transform(GiveUniqueNodeNames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(\"part1_parent.onnx\")\n",
    "\n",
    "\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(\"part1_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(\"part1_dataflow_model.onnx\")\n",
    "fc_layers = model.get_nodes_by_op_type(\"MatrixVectorActivation\")\n",
    "# each tuple is (PE, SIMD, in_fifo_depth,ramstyle) for a layer\n",
    "folding = [\n",
    "    (10, 10, \"auto\"),\n",
    "    (10, 10, \"auto\"),\n",
    "]\n",
    "for fcl, (pe, simd,ramstyle) in zip(fc_layers, folding):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"ram_style\", ramstyle)\n",
    "\n",
    "# use same SIMD values for the sliding window operators\n",
    "swg_layers = model.get_nodes_by_op_type(\"ConvolutionInputGenerator1D\")\n",
    "for i in range(len(swg_layers)):\n",
    "    swg_inst = getCustomOp(swg_layers[i])\n",
    "    simd = folding[i][1]\n",
    "    swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    swg_inst.set_nodeattr(\"ram_style\", ramstyle)\n",
    "\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model.save(\"part1_dataflow_folded.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the names of the supported PYNQ boards\n",
    "from finn.util.basic import pynq_part_map\n",
    "print(pynq_part_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pynq_board = \"Pynq-Z1\"\n",
    "target_clk_ns = 10\n",
    "\n",
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "model = ModelWrapper(\"part1_dataflow_folded.onnx\")\n",
    "model = model.transform(ZynqBuild(platform = test_pynq_board, period_ns = target_clk_ns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
